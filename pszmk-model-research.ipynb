{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "model = models.detection.keypointrcnn_resnet50_fpn(\n",
    "    weights=models.detection.KeypointRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    )\n",
    "\n",
    "# summary(model)\n",
    "models.detection.KeypointRCNN_ResNet50_FPN_Weights.COCO_V1.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12690\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "data_path = Path(current_working_dir) / 'data'\n",
    "\n",
    "print(len(os.listdir(data_path / 'yogaposes-aii22-challenge' / 'Train')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "image_path = data_path / 'yogaposes-aii22-challenge' / 'Train' / '00033.jpg'\n",
    "image = Image.open(image_path)\n",
    "image_np = np.array(image)\n",
    "# print(\"Image size:\", image.size)\n",
    "# print(\"Image format:\", image.format)\n",
    "# print(\"Image mode:\", image.mode)\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = [\n",
    "    \"0 - nose\",\n",
    "    \"1 - left eye (inner)\",\n",
    "    \"2 - left eye\",\n",
    "    \"3 - left eye (outer)\",\n",
    "    \"4 - right eye (inner)\",\n",
    "    \"5 - right eye\",\n",
    "    \"6 - right eye (outer)\",\n",
    "    \"7 - left ear\",\n",
    "    \"8 - right ear\",\n",
    "    \"9 - mouth (left)\",\n",
    "    \"10 - mouth (right)\",\n",
    "    \"11 - left shoulder\",\n",
    "    \"12 - right shoulder\",\n",
    "    \"13 - left elbow\",\n",
    "    \"14 - right elbow\",\n",
    "    \"15 - left wrist\",\n",
    "    \"16 - right wrist\",\n",
    "    \"17 - left pinky\",\n",
    "    \"18 - right pinky\",\n",
    "    \"19 - left index\",\n",
    "    \"20 - right index\",\n",
    "    \"21 - left thumb\",\n",
    "    \"22 - right thumb\",\n",
    "    \"23 - left hip\",\n",
    "    \"24 - right hip\",\n",
    "    \"25 - left knee\",\n",
    "    \n",
    "    \"26 - right knee\",\n",
    "    \"27 - left ankle\",\n",
    "    \"28 - right ankle\",\n",
    "    \"29 - left heel\",\n",
    "    \"30 - right heel\",\n",
    "    \"31 - left foot index\",\n",
    "    \"32 - right foot index\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "# import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize MediaPipe Pose model\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Detect poses in an input image\n",
    "results = pose.process(image_np)#cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Extract normalized keypoints\n",
    "keypoints = results.pose_landmarks.landmark  # List of normalized keypoints (x, y, z)\n",
    "# Assuming keypoints are normalized and represented as (x, y, z) coordinates\n",
    "\n",
    "# Example: Embedding as a flattened feature vector\n",
    "feature_vector = np.array([kp.x for kp in keypoints] + [kp.y for kp in keypoints] + [kp.z for kp in keypoints])\n",
    "\n",
    "# Normalize feature vector if needed\n",
    "# ...\n",
    "\n",
    "# Input embedded features into a classifier model\n",
    "# classifier_model.predict(feature_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MediapipeCroppedDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_files = os.listdir(data_dir)\n",
    "        self.transform = transforms.Compose([\n",
    "            self.custom_transform,\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.transform_crop_proposal = mp.solutions.pose.Pose(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "            )\n",
    "\n",
    "    def custom_transform(self, image):\n",
    "        mp_pose = mp.solutions.pose \n",
    "\n",
    "        image.flags.writeable = False\n",
    "        pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "        result = pose.process(image)\n",
    "\n",
    "        # Create a white background image with the same size as the original image\n",
    "        white_background = np.ones_like(image) * 255\n",
    "\n",
    "        # Make the white background image writeable\n",
    "        white_background.flags.writeable = True\n",
    "    \n",
    "        # Render detections on the white background with specified colors\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing.draw_landmarks(\n",
    "            white_background, result.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=4, circle_radius=4),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2)\n",
    "        )\n",
    "        white_background_bgr = cv2.cvtColor(white_background, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Get landmarks of the detected pose\n",
    "        landmarks = result.pose_landmarks.landmark if result.pose_landmarks else []\n",
    "\n",
    "        # Extract x, y coordinates of landmarks\n",
    "        landmark_coords = np.array([(lm.x, lm.y) for lm in landmarks])\n",
    "        \n",
    "        if landmark_coords.size > 0:\n",
    "            # Find the bounding box that encloses the detected skeleton\n",
    "            min_x, min_y = np.min(landmark_coords, axis=0)\n",
    "            max_x, max_y = np.max(landmark_coords, axis=0)\n",
    "        \n",
    "            # Add 5-pixel margins to the bounding box\n",
    "            margin = 5\n",
    "            min_x -= margin\n",
    "            min_y -= margin\n",
    "            max_x += margin\n",
    "            max_y += margin\n",
    "        \n",
    "            # Crop the image based on the bounding box\n",
    "            cropped_image = white_background_bgr[int(min_y * white_background_bgr.shape[0]):int(max_y * white_background_bgr.shape[0]),\n",
    "                            int(min_x * white_background_bgr.shape[1]):int(max_x * white_background_bgr.shape[1])]\n",
    "        \n",
    "            # Resize the cropped image to a consistent size (e.g., 224x224 for classification)\n",
    "            resized_image = cv2.resize(cropped_image, (224, 224))\n",
    "            resized_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            return resized_image\n",
    "        else:\n",
    "            # Handle the case where no pose landmarks are detected\n",
    "            return np.zeros((224, 224), dtype=np.uint8)  # You can adjust the default image as needed\n",
    "\n",
    "    def get_label(self, img_name):\n",
    "        # Extract the label from the image name (assuming it's the first digit)\n",
    "        label = int(img_name[0])  # Convert the first character to an integer\n",
    "        return label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = cv2.imread(img_name)  # Read image using OpenCV\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        tensor_image = self.transform(image)\n",
    "        label = self.get_label(self.image_files[idx])\n",
    "\n",
    "        # Save the preprocessed image to the \"NewTrain\" folder\n",
    "        save_path = os.path.join(\"NewTrain\", f\"preprocessed_{self.image_files[idx]}\")\n",
    "        if tensor_image.shape[0] == 1:\n",
    "            # If it's a single-channel image, convert it to three channels before saving\n",
    "            tensor_image_rgb = torch.cat([tensor_image] * 3, dim=0)\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(tensor_image_rgb.numpy().transpose(1, 2, 0), cv2.COLOR_RGB2BGR))\n",
    "        else:\n",
    "            # If it's already a three-channel image, save as is\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(tensor_image.numpy().transpose(1, 2, 0), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "        return tensor_image, label\n",
    "\n",
    "# Example usage:\n",
    "data_directory = \"Train\"\n",
    "custom_dataset = CustomDataset(data_directory)\n",
    "\n",
    "# Example loading one image\n",
    "sample_image, label = custom_dataset[2]\n",
    "file = os.listdir(data_directory)\n",
    "print(file[2])\n",
    "print(sample_image.shape)  # Check the shape (should be torch.Size([1, 224, 224]))\n",
    "print(\"Label:\", label)\n",
    "print(sample_image)\n",
    "\n",
    "min_value = torch.min(sample_image)\n",
    "max_value = torch.max(sample_image)\n",
    "\n",
    "print(\"Minimum value:\", min_value.item())\n",
    "print(\"Maximum value:\", max_value.item())\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "numpy_image = sample_image.squeeze().numpy()\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(numpy_image, cmap='gray')  # Use 'gray' colormap for single-channel images\n",
    "plt.title(\"Image Title\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose-estimation-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
